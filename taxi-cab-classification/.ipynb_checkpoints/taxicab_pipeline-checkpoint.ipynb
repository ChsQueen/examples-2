{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "import datetime\n",
    "import urlparse\n",
    "import pandas as pd\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_model_analysis as tfma\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "from apache_beam.io import textio\n",
    "from apache_beam.io import tfrecordio\n",
    "\n",
    "from tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3\n",
    "from tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3_arg_scope\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow_transform.saved import input_fn_maker\n",
    "from tensorflow_transform.saved import saved_transform_io\n",
    "from tensorflow_transform.beam import impl as beam_impl\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "from tensorflow_transform.coders.csv_coder import CsvCoder\n",
    "from tensorflow_transform.coders.example_proto_coder import ExampleProtoCoder\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import metadata_io\n",
    "from tensorflow_transform import coders as tft_coders\n",
    "from tensorflow_model_analysis.slicer import slicer\n",
    "\n",
    "from ipywidgets.embed import embed_data\n",
    "\n",
    "DATA_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "functions"
    ]
   },
   "outputs": [],
   "source": [
    "# from training.py and transform.py\n",
    "def make_tft_input_metadata(schema):\n",
    "    \"\"\"Make a TFT Schema object\n",
    "    In the tft framework, this is where default values are recoreded for training.\n",
    "    Args:\n",
    "      schema: schema list of training data.\n",
    "    Returns:\n",
    "      TFT metadata object.\n",
    "    \"\"\"\n",
    "    tft_schema = {}\n",
    "\n",
    "    for col_schema in schema:\n",
    "        col_type = col_schema['type']\n",
    "        col_name = col_schema['name']\n",
    "        if col_type == 'NUMBER':\n",
    "            tft_schema[col_name] = dataset_schema.ColumnSchema(\n",
    "                tf.float32, [], dataset_schema.FixedColumnRepresentation(default_value=0.0))\n",
    "        elif col_type in ['CATEGORY', 'TEXT', 'IMAGE_URL', 'KEY']:\n",
    "            tft_schema[col_name] = dataset_schema.ColumnSchema(\n",
    "                tf.string, [], dataset_schema.FixedColumnRepresentation(default_value=''))\n",
    "    return dataset_metadata.DatasetMetadata(dataset_schema.Schema(tft_schema))\n",
    "\n",
    "# from processing.py\n",
    "# Categorical features are assumed to each have a maximum value in the dataset.\n",
    "MAX_CATEGORICAL_FEATURE_VALUES = [24, 31, 12]\n",
    "CATEGORICAL_FEATURE_KEYS = [\n",
    "    'trip_start_hour',\n",
    "    'trip_start_day',\n",
    "    'trip_start_month'\n",
    "]\n",
    "\n",
    "DENSE_FLOAT_FEATURE_KEYS = [\n",
    "    'trip_miles',\n",
    "    'fare',\n",
    "    'trip_seconds'\n",
    "]\n",
    "\n",
    "# Number of buckets used by tf.transform for encoding each feature.\n",
    "FEATURE_BUCKET_COUNT = 10\n",
    "\n",
    "BUCKET_FEATURE_KEYS = [\n",
    "    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n",
    "    'dropoff_longitude'\n",
    "]\n",
    "\n",
    "# Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "# Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\n",
    "OOV_SIZE = 10\n",
    "\n",
    "VOCAB_FEATURE_KEYS = [\n",
    "    'pickup_census_tract',\n",
    "    'dropoff_census_tract',\n",
    "    'payment_type',\n",
    "    'company',\n",
    "    'pickup_community_area',\n",
    "    'dropoff_community_area'\n",
    "]\n",
    "LABEL_KEY = 'tips'\n",
    "FARE_KEY = 'fare'\n",
    "\n",
    "\n",
    "def preprocess(inputs):\n",
    "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "    Args:\n",
    "      inputs: map from feature keys to raw not-yet-transformed features.\n",
    "    Returns:\n",
    "      Map from string feature key to transformed feature operations.\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "    for key in DENSE_FLOAT_FEATURE_KEYS:\n",
    "        # Preserve this feature as a dense float, setting nan's to the mean.\n",
    "        outputs[key] = tft.scale_to_z_score(inputs[key])\n",
    "\n",
    "    for key in VOCAB_FEATURE_KEYS:\n",
    "        # Build a vocabulary for this feature.\n",
    "        if inputs[key].dtype == tf.string:\n",
    "            vocab_tensor = inputs[key]\n",
    "        else:\n",
    "            vocab_tensor = tf.as_string(inputs[key])\n",
    "        outputs[key] = tft.string_to_int(\n",
    "            vocab_tensor, vocab_filename='vocab_' + key,\n",
    "            top_k=VOCAB_SIZE, num_oov_buckets=OOV_SIZE)\n",
    "\n",
    "    for key in BUCKET_FEATURE_KEYS:\n",
    "        outputs[key] = tft.bucketize(inputs[key], FEATURE_BUCKET_COUNT)\n",
    "\n",
    "    for key in CATEGORICAL_FEATURE_KEYS:\n",
    "        outputs[key] = tf.to_int64(inputs[key])\n",
    "\n",
    "    taxi_fare = inputs[FARE_KEY]\n",
    "    taxi_tip = inputs[LABEL_KEY]\n",
    "    # Test if the tip was > 20% of the fare.\n",
    "    tip_threshold = tf.multiply(taxi_fare, tf.constant(0.2))\n",
    "    outputs[LABEL_KEY] = tf.logical_and(\n",
    "        tf.logical_not(tf.is_nan(taxi_fare)),\n",
    "        tf.greater(taxi_tip, tip_threshold))\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def get_feature_columns(transformed_data_dir):\n",
    "    \"\"\"Callback that returns a list of feature columns for building a tf.estimator.\n",
    "    Args:\n",
    "      transformed_data_dir: The GCS directory holding the output of the tft transformation.\n",
    "    Returns:\n",
    "      A list of tf.feature_column.\n",
    "    \"\"\"\n",
    "    return (\n",
    "            [tf.feature_column.numeric_column(key, shape=()) for key in DENSE_FLOAT_FEATURE_KEYS] +\n",
    "            [tf.feature_column.indicator_column(\n",
    "                tf.feature_column.categorical_column_with_identity(\n",
    "                    key, num_buckets=VOCAB_SIZE + OOV_SIZE))\n",
    "                for key in VOCAB_FEATURE_KEYS] +\n",
    "            [tf.feature_column.indicator_column(\n",
    "                tf.feature_column.categorical_column_with_identity(\n",
    "                    key, num_buckets=FEATURE_BUCKET_COUNT, default_value=0))\n",
    "                for key in BUCKET_FEATURE_KEYS] +\n",
    "            [tf.feature_column.indicator_column(\n",
    "                tf.feature_column.categorical_column_with_identity(\n",
    "                    key, num_buckets=num_buckets, default_value=0))\n",
    "                for key, num_buckets in zip(CATEGORICAL_FEATURE_KEYS, MAX_CATEGORICAL_FEATURE_VALUES)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Validation Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass: requires updated packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transform Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:transform"
    ]
   },
   "outputs": [],
   "source": [
    "# Inception Checkpoint\n",
    "INCEPTION_V3_CHECKPOINT = 'gs://cloud-ml-data/img/flower_photos/inception_v3_2016_08_28.ckpt'\n",
    "INCEPTION_EXCLUDED_VARIABLES = ['InceptionV3/AuxLogits', 'InceptionV3/Logits', 'global_step']\n",
    "\n",
    "DELIMITERS = '.,!?() '\n",
    "\n",
    "\n",
    "def _image_to_vec(image_str_tensor):\n",
    "    def _decode_and_resize(image_str_tensor):\n",
    "        \"\"\"Decodes jpeg string, resizes it and returns a uint8 tensor.\"\"\"\n",
    "\n",
    "        # These constants are set by Inception v3's expectations.\n",
    "        height = 299\n",
    "        width = 299\n",
    "        channels = 3\n",
    "\n",
    "        image = tf.read_file(image_str_tensor)\n",
    "        image = tf.image.decode_jpeg(image, channels=channels)\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        image = tf.image.resize_bilinear(image, [height, width], align_corners=False)\n",
    "        image = tf.squeeze(image, squeeze_dims=[0])\n",
    "        image = tf.cast(image, dtype=tf.uint8)\n",
    "        return image\n",
    "\n",
    "    image = tf.map_fn(_decode_and_resize, image_str_tensor, back_prop=False, dtype=tf.uint8)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    image = tf.subtract(image, 0.5)\n",
    "    inception_input = tf.multiply(image, 2.0)\n",
    "\n",
    "    # Build Inception layers, which expect a tensor of type float from [-1, 1)\n",
    "    # and shape [batch_size, height, width, channels].\n",
    "    with tf.contrib.slim.arg_scope(inception_v3_arg_scope()):\n",
    "        _, end_points = inception_v3(inception_input, is_training=False)\n",
    "\n",
    "    embeddings = end_points['PreLogits']\n",
    "    inception_embeddings = tf.squeeze(embeddings, [1, 2], name='SpatialSqueeze')\n",
    "    return inception_embeddings\n",
    "\n",
    "\n",
    "def make_preprocessing_fn(schema):\n",
    "    \"\"\"Makes a preprocessing function.\n",
    "    Args:\n",
    "      schema: the schema of the training data.\n",
    "    Returns:\n",
    "      a preprocessing_fn function used by tft.\n",
    "    \"\"\"\n",
    "\n",
    "    def preprocessing_fn(inputs):\n",
    "        \"\"\"TFT preprocessing function.\n",
    "        Args:\n",
    "          inputs: dictionary of input `tensorflow_transform.Column`.\n",
    "        Returns:\n",
    "          A dictionary of `tensorflow_transform.Column` representing the transformed\n",
    "              columns.\n",
    "        \"\"\"\n",
    "\n",
    "        features_dict = {}\n",
    "        for col_schema in schema:\n",
    "            col_name = col_schema['name']\n",
    "            if col_schema['type'] == 'NUMBER':\n",
    "                features_dict[col_name] = inputs[col_name]\n",
    "            elif col_schema['type'] == 'CATEGORY':\n",
    "                features_dict[col_name] = tft.string_to_int(inputs[col_name],\n",
    "                                                            vocab_filename='vocab_' + col_name)\n",
    "            elif col_schema['type'] == 'TEXT':\n",
    "                tokens = tf.string_split(inputs[col_name], DELIMITERS)\n",
    "                # TODO: default_value = 0 is wrong. It means OOV gets 0 for their index.\n",
    "                # But this is to workaround the issue that trainer can use the true vocab\n",
    "                # size. Otherwise trainer has to use VOCAB_SIZE defined in this file which\n",
    "                # is too large. I am talking to TFT folks on this. If there is no workaround,\n",
    "                # user has to provide a vocab_size.\n",
    "                VOCAB_SIZE = 100000\n",
    "                indices = tft.string_to_int(tokens,\n",
    "                                            vocab_filename='vocab_' + col_name,\n",
    "                                            default_value=0)\n",
    "                # Add one for the oov bucket created by string_to_int.\n",
    "                bow_indices, bow_weights = tft.tfidf(indices, VOCAB_SIZE + 1)\n",
    "                features_dict[col_name + '_indices'] = bow_indices\n",
    "                features_dict[col_name + '_weights'] = bow_weights\n",
    "            elif col_schema['type'] == 'IMAGE_URL':\n",
    "                features_dict[col_name] = tft.apply_function_with_checkpoint(\n",
    "                    _image_to_vec,\n",
    "                    [inputs[col_name]],\n",
    "                    INCEPTION_V3_CHECKPOINT,\n",
    "                    exclude=INCEPTION_EXCLUDED_VARIABLES)\n",
    "            elif col_schema['type'] == 'KEY':\n",
    "                features_dict[col_name] = inputs[col_name]\n",
    "            else:\n",
    "                raise ValueError('Invalid schema. Unknown type ' + col_schema['type'])\n",
    "        return features_dict\n",
    "\n",
    "    return preprocessing_fn\n",
    "\n",
    "\n",
    "def run_transform(output_dir, schema, train_data_file, eval_data_file, preprocessing_fn=None):\n",
    "    \"\"\"Writes a tft transform fn, and metadata files.\n",
    "    Args:\n",
    "      output_dir: output folder\n",
    "      schema: schema list.\n",
    "      train_data_file: training data file pattern.\n",
    "      eval_data_file: eval data file pattern.\n",
    "      preprocessing_fn: a function used to preprocess the raw data. If not\n",
    "                        specified, a function will be automatically inferred\n",
    "                        from the schema.\n",
    "    \"\"\"\n",
    "\n",
    "    tft_input_metadata = make_tft_input_metadata(schema)\n",
    "    temp_dir = os.path.join(output_dir, 'tmp')\n",
    "    preprocessing_fn = preprocessing_fn or make_preprocessing_fn(schema)\n",
    "\n",
    "    runner = 'DirectRunner'\n",
    "    with beam.Pipeline(runner, options=None) as p:\n",
    "        with beam_impl.Context(temp_dir=temp_dir):\n",
    "            names = [x['name'] for x in schema]\n",
    "            converter = CsvCoder(names, tft_input_metadata.schema)\n",
    "            train_data = (\n",
    "                    p\n",
    "                    | 'ReadTrainData' >> textio.ReadFromText(train_data_file)\n",
    "                    | 'DecodeTrainData' >> beam.Map(converter.decode))\n",
    "\n",
    "            train_dataset = (train_data, tft_input_metadata)\n",
    "            transformed_dataset, transform_fn = (\n",
    "                    train_dataset | beam_impl.AnalyzeAndTransformDataset(preprocessing_fn))\n",
    "            transformed_data, transformed_metadata = transformed_dataset\n",
    "\n",
    "            # Writes transformed_metadata and transfrom_fn folders\n",
    "            _ = (transform_fn | 'WriteTransformFn' >> transform_fn_io.WriteTransformFn(output_dir))\n",
    "\n",
    "            # Write the raw_metadata\n",
    "            metadata_io.write_metadata(\n",
    "                metadata=tft_input_metadata,\n",
    "                path=os.path.join(output_dir, 'metadata'))\n",
    "\n",
    "            _ = transformed_data | 'WriteTrainData' >> tfrecordio.WriteToTFRecord(\n",
    "                os.path.join(output_dir, 'train'),\n",
    "                coder=ExampleProtoCoder(transformed_metadata.schema))\n",
    "\n",
    "            eval_data = (\n",
    "                    p\n",
    "                    | 'ReadEvalData' >> textio.ReadFromText(eval_data_file)\n",
    "                    | 'DecodeEvalData' >> beam.Map(converter.decode))\n",
    "\n",
    "            eval_dataset = (eval_data, tft_input_metadata)\n",
    "\n",
    "            transformed_eval_dataset = (\n",
    "                    (eval_dataset, transform_fn) | beam_impl.TransformDataset())\n",
    "            transformed_eval_data, transformed_metadata = transformed_eval_dataset\n",
    "\n",
    "            _ = transformed_eval_data | 'WriteEvalData' >> tfrecordio.WriteToTFRecord(\n",
    "                os.path.join(output_dir, 'eval'),\n",
    "                coder=ExampleProtoCoder(transformed_metadata.schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA = 'taxi-cab-classification/train.csv'\n",
    "EVALUATION_DATA = 'taxi-cab-classification/eval.csv'\n",
    "\n",
    "if os.path.exists(os.path.join(DATA_DIR, \"transformed\")):\n",
    "    shutil.rmtree(os.path.join(DATA_DIR, \"transformed\"))\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "schema = json.loads(file_io.read_file_to_string(os.path.join(DATA_DIR, \"taxi-cab-classification/schema.json\")))\n",
    "\n",
    "def wrapped_preprocessing_fn(inputs):\n",
    "    outputs = preprocess(inputs)\n",
    "    for key in outputs:\n",
    "        if outputs[key].dtype == tf.bool:\n",
    "            outputs[key] = tft.string_to_int(tf.as_string(outputs[key]),\n",
    "                                             vocab_filename='vocab_' + key)\n",
    "    return outputs\n",
    "preprocessing_fn = wrapped_preprocessing_fn\n",
    "\n",
    "run_transform(os.path.join(DATA_DIR, \"transformed\"),\n",
    "              schema,\n",
    "              DATA_DIR + TRAIN_DATA,\n",
    "              DATA_DIR + EVALUATION_DATA,\n",
    "              preprocessing_fn=preprocessing_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:train",
     "prev:transform"
    ]
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "HIDDEN_LAYER_SIZE = '1500'\n",
    "STEPS = 3\n",
    "CLASSIFICATION_TARGET_TYPES = [tf.bool, tf.int32, tf.int64]\n",
    "REGRESSION_TARGET_TYPES = [tf.float32, tf.float64]\n",
    "TARGET_TYPES = CLASSIFICATION_TARGET_TYPES + REGRESSION_TARGET_TYPES\n",
    "\n",
    "\n",
    "def is_classification(transformed_data_dir, target):\n",
    "    \"\"\"Whether the scenario is classification (vs regression).\n",
    "\n",
    "    Returns:\n",
    "      The number of classes if the target represents a classification\n",
    "      problem, or None if it does not.\n",
    "    \"\"\"\n",
    "    transformed_metadata = metadata_io.read_metadata(\n",
    "        os.path.join(transformed_data_dir, transform_fn_io.TRANSFORMED_METADATA_DIR))\n",
    "    transformed_feature_spec = transformed_metadata.schema.as_feature_spec()\n",
    "    if target not in transformed_feature_spec:\n",
    "        raise ValueError('Cannot find target \"%s\" in transformed data.' % target)\n",
    "\n",
    "    feature = transformed_feature_spec[target]\n",
    "    if (not isinstance(feature, tf.FixedLenFeature) or feature.shape != [] or\n",
    "            feature.dtype not in TARGET_TYPES):\n",
    "        raise ValueError('target \"%s\" is of invalid type.' % target)\n",
    "\n",
    "    if feature.dtype in CLASSIFICATION_TARGET_TYPES:\n",
    "        if feature.dtype == tf.bool:\n",
    "            return 2\n",
    "        return get_vocab_size(transformed_data_dir, target)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def make_training_input_fn(transformed_data_dir, mode, batch_size, target_name, num_epochs=None):\n",
    "    \"\"\"Creates an input function reading from transformed data.\n",
    "    Args:\n",
    "      transformed_data_dir: Directory to read transformed data and metadata from.\n",
    "      mode: 'train' or 'eval'.\n",
    "      batch_size: Batch size.\n",
    "      target_name: name of the target column.\n",
    "      num_epochs: number of training data epochs.\n",
    "    Returns:\n",
    "      The input function for training or eval.\n",
    "    \"\"\"\n",
    "    transformed_metadata = metadata_io.read_metadata(\n",
    "        os.path.join(transformed_data_dir, transform_fn_io.TRANSFORMED_METADATA_DIR))\n",
    "    transformed_feature_spec = transformed_metadata.schema.as_feature_spec()\n",
    "\n",
    "    def _input_fn():\n",
    "        \"\"\"Input function for training and eval.\"\"\"\n",
    "        epochs = 1 if mode == 'eval' else num_epochs\n",
    "        transformed_features = tf.contrib.learn.io.read_batch_features(\n",
    "            os.path.join(transformed_data_dir, mode + '-*'),\n",
    "            batch_size, transformed_feature_spec, tf.TFRecordReader, num_epochs=epochs)\n",
    "\n",
    "        # Extract features and label from the transformed tensors.\n",
    "        transformed_labels = transformed_features.pop(target_name)\n",
    "        return transformed_features, transformed_labels\n",
    "\n",
    "    return _input_fn\n",
    "\n",
    "\n",
    "def make_serving_input_fn(transformed_data_dir, schema, target_name):\n",
    "    \"\"\"Creates an input function reading from transformed data.\n",
    "    Args:\n",
    "      transformed_data_dir: Directory to read transformed data and metadata from.\n",
    "      schema: the raw data schema.\n",
    "      target_name: name of the target column.\n",
    "    Returns:\n",
    "      The input function for serving.\n",
    "    \"\"\"\n",
    "    raw_metadata = make_tft_input_metadata(schema)\n",
    "    raw_feature_spec = raw_metadata.schema.as_feature_spec()\n",
    "\n",
    "    raw_keys = [x['name'] for x in schema]\n",
    "    raw_keys.remove(target_name)\n",
    "    serving_input_fn = input_fn_maker.build_csv_transforming_serving_input_receiver_fn(\n",
    "        raw_metadata=raw_metadata,\n",
    "        transform_savedmodel_dir=transformed_data_dir + '/transform_fn',\n",
    "        raw_keys=raw_keys)\n",
    "\n",
    "    return serving_input_fn\n",
    "\n",
    "\n",
    "def get_vocab_size(transformed_data_dir, feature_name):\n",
    "    \"\"\"Get vocab size of a given text or category column.\"\"\"\n",
    "    vocab_file = os.path.join(transformed_data_dir,\n",
    "                              transform_fn_io.TRANSFORM_FN_DIR,\n",
    "                              'assets',\n",
    "                              'vocab_' + feature_name)\n",
    "    with file_io.FileIO(vocab_file, 'r') as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "\n",
    "def get_estimator(schema, transformed_data_dir, target_name, output_dir, hidden_units,\n",
    "                  optimizer, learning_rate, feature_columns):\n",
    "    \"\"\"Get proper tf.estimator (DNNClassifier or DNNRegressor).\"\"\"\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "    if optimizer == 'Adam':\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    elif optimizer == 'SGD':\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "    # Set how often to run checkpointing in terms of steps.\n",
    "    config = tf.contrib.learn.RunConfig(save_checkpoints_steps=1000)\n",
    "    n_classes = is_classification(transformed_data_dir, target_name)\n",
    "    if n_classes:\n",
    "        estimator = tf.estimator.DNNClassifier(\n",
    "            feature_columns=feature_columns,\n",
    "            hidden_units=hidden_units,\n",
    "            n_classes=n_classes,\n",
    "            config=config,\n",
    "            model_dir=output_dir)\n",
    "    else:\n",
    "        estimator = tf.estimator.DNNRegressor(\n",
    "            feature_columns=feature_columns,\n",
    "            hidden_units=hidden_units,\n",
    "            config=config,\n",
    "            model_dir=output_dir,\n",
    "            optimizer=optimizer)\n",
    "\n",
    "    return estimator\n",
    "\n",
    "\n",
    "def eval_input_receiver_fn(tf_transform_dir, schema, target):\n",
    "    \"\"\"Build everything needed for the tf-model-analysis to run the model.\n",
    "    Args:\n",
    "      tf_transform_dir: directory in which the tf-transform model was written\n",
    "        during the preprocessing step.\n",
    "      schema: the raw data schema.\n",
    "      target: name of the target column.\n",
    "    Returns:\n",
    "      EvalInputReceiver function, which contains:\n",
    "        - Tensorflow graph which parses raw untranformed features, applies the\n",
    "          tf-transform preprocessing operators.\n",
    "        - Set of raw, untransformed features.\n",
    "        - Label against which predictions will be compared.\n",
    "    \"\"\"\n",
    "    raw_metadata = make_tft_input_metadata(schema)\n",
    "    raw_feature_spec = raw_metadata.schema.as_feature_spec()\n",
    "    serialized_tf_example = tf.placeholder(\n",
    "        dtype=tf.string, shape=[None], name='input_example_tensor')\n",
    "    features = tf.parse_example(serialized_tf_example, raw_feature_spec)\n",
    "    _, transformed_features = (\n",
    "        saved_transform_io.partially_apply_saved_transform(\n",
    "            os.path.join(tf_transform_dir, transform_fn_io.TRANSFORM_FN_DIR),\n",
    "            features))\n",
    "    receiver_tensors = {'examples': serialized_tf_example}\n",
    "    return tfma.export.EvalInputReceiver(\n",
    "        features=transformed_features,\n",
    "        receiver_tensors=receiver_tensors,\n",
    "        labels=transformed_features[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "hidden_layer_size = [int(x.strip()) for x in HIDDEN_LAYER_SIZE.split(',')]\n",
    "schema = json.loads(file_io.read_file_to_string(os.path.join(DATA_DIR, \"taxi-cab-classification/schema.json\")))\n",
    "feature_columns = None\n",
    "feature_columns = get_feature_columns(os.path.join(DATA_DIR, \"transformed\"))\n",
    "estimator = get_estimator(schema, \n",
    "                          os.path.join(DATA_DIR, \"transformed\"), \n",
    "                          \"tips\",\n",
    "                          os.path.join(DATA_DIR, \"training\"),\n",
    "                          hidden_layer_size,\n",
    "                          \"Adagrad\", \n",
    "                          LEARNING_RATE,\n",
    "                          feature_columns)\n",
    "\n",
    "# TODO: Expose batch size.\n",
    "train_input_fn = make_training_input_fn(\n",
    "    os.path.join(DATA_DIR, \"transformed\"),\n",
    "    'train',\n",
    "    32,\n",
    "    \"tips\",\n",
    "    num_epochs=2)\n",
    "\n",
    "eval_input_fn = make_training_input_fn(\n",
    "    os.path.join(DATA_DIR, \"transformed\"),\n",
    "    'eval',\n",
    "    32,\n",
    "    \"tips\")\n",
    "serving_input_fn = make_serving_input_fn(\n",
    "    os.path.join(DATA_DIR, \"transformed\"),\n",
    "    schema,\n",
    "    \"tips\")\n",
    "\n",
    "exporter = tf.estimator.FinalExporter('export', serving_input_fn)\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=STEPS)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, exporters=[exporter])\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "eval_model_dir = os.path.join(DATA_DIR + \"training\", 'tfma_eval_model_dir')\n",
    "tfma.export.export_eval_savedmodel(\n",
    "    estimator=estimator,\n",
    "    export_dir_base=eval_model_dir,\n",
    "    eval_input_receiver_fn=(\n",
    "        lambda: eval_input_receiver_fn(\n",
    "            os.path.join(DATA_DIR, \"transformed\"), schema, \"tips\")))\n",
    "\n",
    "metadata = {\n",
    "    'outputs': [{\n",
    "        'type': 'tensorboard',\n",
    "        'source': os.path.join(DATA_DIR, \"training\"),\n",
    "    }]\n",
    "}\n",
    "\n",
    "try:\n",
    "    os.makedirs(os.path.join(DATA_DIR, \"artifacts\", \"training\"))\n",
    "except OSError:\n",
    "    pass\n",
    "with open(os.path.join(DATA_DIR, \"artifacts\", \"training\", 'mlpipeline-ui-metadata.json'), 'w') as f:\n",
    "    json.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:analyze",
     "prev:train"
    ]
   },
   "outputs": [],
   "source": [
    "_OUTPUT_HTML_FILE = 'output_display.html'\n",
    "_STATIC_HTML_TEMPLATE = \"\"\"\n",
    "<html>\n",
    "  <head>\n",
    "    <title>TFMA Slicing Metrics</title>\n",
    "\n",
    "    <!-- Load RequireJS, used by the IPywidgets for dependency management -->\n",
    "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js\"\n",
    "            integrity=\"sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=\"\n",
    "            crossorigin=\"anonymous\">\n",
    "    </script>\n",
    "\n",
    "    <!-- Load IPywidgets bundle for embedding. -->\n",
    "    <script src=\"https://unpkg.com/@jupyter-widgets/html-manager@^0.12.0/dist/embed-amd.js\"\n",
    "            crossorigin=\"anonymous\">\n",
    "    </script>\n",
    "\n",
    "    <!-- Load IPywidgets bundle for embedding. -->\n",
    "    <script>\n",
    "      require.config({{\n",
    "        paths: {{\n",
    "          \"tfma_widget_js\": \"https://cdn.rawgit.com/tensorflow/model-analysis/v0.6.0/tensorflow_model_analysis/static/index\"\n",
    "        }}\n",
    "      }});\n",
    "    </script>\n",
    "\n",
    "    <link rel=\"import\" href=\"https://cdn.rawgit.com/tensorflow/model-analysis/v0.6.0/tensorflow_model_analysis/static/vulcanized_template.html\">\n",
    "\n",
    "    <!-- The state of all the widget models on the page -->\n",
    "    <script type=\"application/vnd.jupyter.widget-state+json\">\n",
    "      {manager_state}\n",
    "    </script>\n",
    "  </head>\n",
    "\n",
    "  <body>\n",
    "    <h1>TFMA Slicing Metrics</h1>\n",
    "    {widget_views}\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "_SINGLE_WIDGET_TEMPLATE = \"\"\"\n",
    "    <div id=\"slicing-metrics-widget-{0}\">\n",
    "      <script type=\"application/vnd.jupyter.widget-view+json\">\n",
    "        {1}\n",
    "      </script>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_raw_feature_spec(schema):\n",
    "    feature_spec = {}\n",
    "    for column in schema:\n",
    "        column_name = column['name']\n",
    "        column_type = column['type']\n",
    "\n",
    "        feature = tf.FixedLenFeature(shape=[], dtype=tf.string, default_value='')\n",
    "        if column_type == 'NUMBER':\n",
    "            feature = tf.FixedLenFeature(shape=[], dtype=tf.float32, default_value=0.0)\n",
    "        feature_spec[column_name] = feature\n",
    "    return feature_spec\n",
    "\n",
    "\n",
    "def clean_raw_data_dict(raw_feature_spec):\n",
    "    def clean_method(input_dict):\n",
    "        output_dict = {}\n",
    "\n",
    "        for key in raw_feature_spec:\n",
    "            if key not in input_dict or not input_dict[key]:\n",
    "                output_dict[key] = raw_feature_spec[key].default_value\n",
    "            else:\n",
    "                output_dict[key] = input_dict[key]\n",
    "        return output_dict\n",
    "\n",
    "    return clean_method\n",
    "\n",
    "\n",
    "def run_analysis(output_dir, model_dir, eval_path, schema, slice_columns):\n",
    "    runner = 'DirectRunner'\n",
    "    column_names = [x['name'] for x in schema]\n",
    "    for slice_column in slice_columns:\n",
    "        if slice_column not in column_names:\n",
    "            raise ValueError(\"Unknown slice column: %s\" % slice_column)\n",
    "\n",
    "    slice_spec = [\n",
    "        slicer.SingleSliceSpec(),  # An empty spec is required for the 'Overall' slice\n",
    "        slicer.SingleSliceSpec(columns=slice_columns)\n",
    "    ]\n",
    "\n",
    "    with beam.Pipeline(runner=runner, options=None) as pipeline:\n",
    "        raw_feature_spec = get_raw_feature_spec(schema)\n",
    "        raw_schema = dataset_schema.from_feature_spec(raw_feature_spec)\n",
    "        example_coder = tft_coders.example_proto_coder.ExampleProtoCoder(raw_schema)\n",
    "        csv_coder = tft_coders.CsvCoder(column_names, raw_schema)\n",
    "\n",
    "        raw_data = (\n",
    "                pipeline\n",
    "                | 'ReadFromText' >> beam.io.ReadFromText(eval_path)\n",
    "                | 'ParseCSV' >> beam.Map(csv_coder.decode)\n",
    "                | 'CleanData' >> beam.Map(clean_raw_data_dict(raw_feature_spec))\n",
    "                | 'ToSerializedTFExample' >> beam.Map(example_coder.encode)\n",
    "                | 'EvaluateAndWriteResults' >> tfma.EvaluateAndWriteResults(\n",
    "            eval_saved_model_path=model_dir,\n",
    "            slice_spec=slice_spec,\n",
    "            output_path=output_dir))\n",
    "\n",
    "\n",
    "def generate_static_html_output(output_dir, slicing_columns):\n",
    "    result = tfma.load_eval_result(output_path=output_dir)\n",
    "    slicing_metrics_views = [\n",
    "        tfma.view.render_slicing_metrics(result, slicing_column=slicing_column)\n",
    "        for slicing_column in slicing_columns\n",
    "    ]\n",
    "    data = embed_data(views=slicing_metrics_views)\n",
    "    manager_state = json.dumps(data['manager_state'])\n",
    "    widget_views = [json.dumps(view) for view in data['view_specs']]\n",
    "    views_html = \"\"\n",
    "    for idx, view in enumerate(widget_views):\n",
    "        views_html += _SINGLE_WIDGET_TEMPLATE.format(idx, view)\n",
    "    rendered_template = _STATIC_HTML_TEMPLATE.format(\n",
    "        manager_state=manager_state, widget_views=views_html)\n",
    "    static_html_path = os.path.join(output_dir, _OUTPUT_HTML_FILE)\n",
    "    file_io.write_string_to_file(static_html_path, rendered_template)\n",
    "\n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage': 'gcs',\n",
    "            'source': static_html_path,\n",
    "        }]\n",
    "    }\n",
    "    try:\n",
    "        os.makedirs(os.path.join(DATA_DIR, \"artifacts\", \"analyze\"))\n",
    "    except OSError:\n",
    "        pass\n",
    "    with file_io.FileIO(os.path.join(DATA_DIR, 'artifacts', 'analyze', 'mlpipeline-ui-metadata.json'), 'w') as f:\n",
    "        json.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLICE_COLUMN = ['trip_start_hour']\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "slice_columns = [\n",
    "    column\n",
    "    for column_group in SLICE_COLUMN\n",
    "    for column in column_group.split(',')\n",
    "]\n",
    "schema = json.loads(file_io.read_file_to_string(os.path.join(DATA_DIR, \"taxi-cab-classification/schema.json\")))\n",
    "eval_model_parent_dir = os.path.join(DATA_DIR, \"training\", 'tfma_eval_model_dir')\n",
    "model_export_dir = os.path.join(eval_model_parent_dir, file_io.list_directory(eval_model_parent_dir)[0])\n",
    "run_analysis(os.path.join(DATA_DIR, \"analysis\"),\n",
    "             model_export_dir,\n",
    "             DATA_DIR + EVALUATION_DATA,\n",
    "             schema,\n",
    "             slice_columns)\n",
    "generate_static_html_output(os.path.join(DATA_DIR, \"analysis\"), slice_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predict Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:predict",
     "prev:train"
    ]
   },
   "outputs": [],
   "source": [
    "class EmitAsBatchDoFn(beam.DoFn):\n",
    "    \"\"\"A DoFn that buffers the records and emits them batch by batch.\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size):\n",
    "        self._batch_size = batch_size\n",
    "        self._cached = []\n",
    "\n",
    "    def process(self, element):\n",
    "        from apache_beam.transforms import window\n",
    "        from apache_beam.utils.windowed_value import WindowedValue\n",
    "        self._cached.append(element)\n",
    "        if len(self._cached) >= self._batch_size:\n",
    "            emit = self._cached\n",
    "            self._cached = []\n",
    "            yield emit\n",
    "\n",
    "    def finish_bundle(self, context=None):\n",
    "        from apache_beam.transforms import window\n",
    "        from apache_beam.utils.windowed_value import WindowedValue\n",
    "        if len(self._cached) > 0:\n",
    "            yield WindowedValue(self._cached, -1, [window.GlobalWindow()])\n",
    "\n",
    "\n",
    "class TargetToLastDoFn(beam.DoFn):\n",
    "    \"\"\"A DoFn that moves specified target column to last.\"\"\"\n",
    "\n",
    "    def __init__(self, names, target_name):\n",
    "        self._names = names\n",
    "        self._target_name = target_name\n",
    "        self._names_no_target = list(names)\n",
    "        self._names_no_target.remove(target_name)\n",
    "\n",
    "    def process(self, element):\n",
    "        import csv\n",
    "        content = csv.DictReader([element], fieldnames=self._names).next()\n",
    "        target = content.pop(self._target_name)\n",
    "        yield [content[x] for x in self._names_no_target] + [target]\n",
    "\n",
    "\n",
    "class PredictDoFn(beam.DoFn):\n",
    "    \"\"\"A DoFn that performs predictions with given trained model.\"\"\"\n",
    "\n",
    "    def __init__(self, model_export_dir):\n",
    "        self._model_export_dir = model_export_dir\n",
    "\n",
    "    def start_bundle(self):\n",
    "        from tensorflow.contrib import predictor\n",
    "\n",
    "        # We need to import the tensorflow_transform library in order to\n",
    "        # register all of the ops that might be used by a saved model that\n",
    "        # incorporates TFT transformations.\n",
    "        import tensorflow_transform\n",
    "\n",
    "        self._predict_fn = predictor.from_saved_model(self._model_export_dir)\n",
    "\n",
    "    def process(self, element):\n",
    "        import csv\n",
    "        import StringIO\n",
    "\n",
    "        prediction_inputs = []\n",
    "        for instance in element:\n",
    "            instance_copy = list(instance)\n",
    "            instance_copy.pop()  # remove target\n",
    "            buf = StringIO.StringIO()\n",
    "            writer = csv.writer(buf, lineterminator='')\n",
    "            writer.writerow(instance_copy)\n",
    "            prediction_inputs.append(buf.getvalue())\n",
    "\n",
    "        return_dict = self._predict_fn({\"inputs\": prediction_inputs})\n",
    "        return_dict['source'] = element\n",
    "        yield return_dict\n",
    "\n",
    "\n",
    "class ListToCsvDoFn(beam.DoFn):\n",
    "    \"\"\"A DoFn function that convert list to csv line.\"\"\"\n",
    "\n",
    "    def process(self, element):\n",
    "        import csv\n",
    "        import StringIO\n",
    "        buf = StringIO.StringIO()\n",
    "        writer = csv.writer(buf, lineterminator='')\n",
    "        writer.writerow(element)\n",
    "        yield buf.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predict(output_dir, data_path, schema, target_name, model_export_dir, batch_size):\n",
    "    \"\"\"Run predictions with given model using DataFlow.\n",
    "    Args:\n",
    "      output_dir: output folder\n",
    "      data_path: test data file path.\n",
    "      schema: schema list.\n",
    "      target_name: target column name.\n",
    "      model_export_dir: GCS or local path of exported model trained with tft preprocessed data.\n",
    "      batch_size: batch size when running prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    target_type = next(x for x in schema if x['name'] == target_name)['type']\n",
    "    labels_file = os.path.join(model_export_dir, 'assets', 'vocab_' + target_name)\n",
    "    is_classification = file_io.file_exists(labels_file)\n",
    "\n",
    "    output_file_prefix = os.path.join(output_dir, 'prediction_results')\n",
    "    output_schema_file = os.path.join(output_dir, 'schema.json')\n",
    "    names = [x['name'] for x in schema]\n",
    "\n",
    "    output_schema = filter(lambda x: x['name'] != target_name, schema)\n",
    "    if is_classification:\n",
    "        with file_io.FileIO(labels_file, mode='r') as f:\n",
    "            labels = [x.strip() for x in f.readlines()]\n",
    "\n",
    "        output_schema.append({'name': 'target', 'type': 'CATEGORY'})\n",
    "        output_schema.append({'name': 'predicted', 'type': 'CATEGORY'})\n",
    "        output_schema.extend([{'name': x, 'type': 'NUMBER'} for x in labels])\n",
    "    else:\n",
    "        output_schema.append({'name': 'target', 'type': 'NUMBER'})\n",
    "        output_schema.append({'name': 'predicted', 'type': 'NUMBER'})\n",
    "\n",
    "    runner = 'DirectRunner'\n",
    "    with beam.Pipeline(runner, options=None) as p:\n",
    "        raw_results = (p\n",
    "                       | 'read data' >> beam.io.ReadFromText(data_path)\n",
    "                       | 'move target to last' >> beam.ParDo(TargetToLastDoFn(names, target_name))\n",
    "                       | 'batch' >> beam.ParDo(EmitAsBatchDoFn(batch_size))\n",
    "                       | 'predict' >> beam.ParDo(PredictDoFn(model_export_dir)))\n",
    "\n",
    "        if is_classification:\n",
    "            processed_results = (raw_results\n",
    "                                 | 'unbatch' >> beam.FlatMap(lambda x: zip(x['source'], x['scores']))\n",
    "                                 | 'get predicted' >> beam.Map(lambda x: x[0] + [labels[x[1].argmax()]] + list(x[1])))\n",
    "        else:\n",
    "            processed_results = (raw_results\n",
    "                                 | 'unbatch' >> beam.FlatMap(lambda x: zip(x['source'], x['outputs']))\n",
    "                                 | 'get predicted' >> beam.Map(lambda x: x[0] + list(x[1])))\n",
    "\n",
    "        results_save = (processed_results\n",
    "                        | 'write csv lines' >> beam.ParDo(ListToCsvDoFn())\n",
    "                        | 'write file' >> beam.io.WriteToText(output_file_prefix))\n",
    "\n",
    "        (results_save\n",
    "         | 'fixed one' >> beam.transforms.combiners.Sample.FixedSizeGlobally(1)\n",
    "         | 'set schema' >> beam.Map(lambda path: json.dumps(output_schema))\n",
    "         | 'write schema file' >> beam.io.WriteToText(output_schema_file, shard_name_template=''))\n",
    "\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# Models trained with estimator are exported to base/export/export/123456781 directory.\n",
    "# Our trainer export only one model.\n",
    "model = os.path.join(DATA_DIR, 'training')  # args.model\n",
    "export_parent_dir = os.path.join(model, 'export', 'export')\n",
    "model_export_dir = os.path.join(export_parent_dir, file_io.list_directory(export_parent_dir)[0])\n",
    "schema = json.loads(file_io.read_file_to_string(os.path.join(DATA_DIR, \"taxi-cab-classification/schema.json\")))\n",
    "\n",
    "batchsize = 32\n",
    "run_predict(os.path.join(DATA_DIR, \"predict\"),\n",
    "            os.path.join(DATA_DIR, EVALUATION_DATA),\n",
    "            schema,\n",
    "            \"tips\",\n",
    "            model_export_dir,\n",
    "            batchsize)\n",
    "prediction_results = os.path.join(DATA_DIR + \"predict\", 'prediction_results-*')\n",
    "\n",
    "with file_io.FileIO(os.path.join(DATA_DIR, \"predict\", 'schema.json'), 'r') as f:\n",
    "    schema = json.load(f)\n",
    "\n",
    "metadata = {\n",
    "    'outputs': [{\n",
    "        'type': 'table',\n",
    "        'storage': 'gcs',\n",
    "        'format': 'csv',\n",
    "        'header': [x['name'] for x in schema],\n",
    "        'source': prediction_results\n",
    "    }]\n",
    "}\n",
    "try:\n",
    "    os.makedirs(os.path.join(DATA_DIR, \"artifacts\", \"predict\"))\n",
    "except OSError:\n",
    "    pass\n",
    "with open(os.path.join(DATA_DIR, 'artifacts', 'predict', 'mlpipeline-ui-metadata.json'), 'w') as f:\n",
    "    json.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:confusionmatrix",
     "prev:predict"
    ]
   },
   "outputs": [],
   "source": [
    "predictions = os.path.join(DATA_DIR, 'predict', 'prediction_results-*')\n",
    "target_lambda = \"\"\"lambda x: (x['target'] > x['fare'] * 0.2)\"\"\"\n",
    "if not os.path.exists(os.path.join(DATA_DIR, \"confusionmatrix\")):\n",
    "    os.makedirs(os.path.join(DATA_DIR, \"confusionmatrix\"))\n",
    "\n",
    "schema_file = os.path.join(os.path.dirname(predictions), 'schema.json')\n",
    "schema = json.loads(file_io.read_file_to_string(schema_file))\n",
    "names = [x['name'] for x in schema]\n",
    "dfs = []\n",
    "files = file_io.get_matching_files(predictions)\n",
    "for file in files:\n",
    "    with file_io.FileIO(file, 'r') as f:\n",
    "        dfs.append(pd.read_csv(f, names=names))\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "if target_lambda:\n",
    "    df['target'] = df.apply(eval(target_lambda), axis=1)\n",
    "\n",
    "vocab = list(df['target'].unique())\n",
    "cm = confusion_matrix(df['target'], df['predicted'], labels=vocab)\n",
    "data = []\n",
    "for target_index, target_row in enumerate(cm):\n",
    "    for predicted_index, count in enumerate(target_row):\n",
    "        data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "\n",
    "df_cm = pd.DataFrame(data, columns=['target', 'predicted', 'count'])\n",
    "cm_file = os.path.join(DATA_DIR, 'confusionmatrix', 'confusion_matrix.csv')\n",
    "with file_io.FileIO(cm_file, 'w') as f:\n",
    "    df_cm.to_csv(f, columns=['target', 'predicted', 'count'], header=False, index=False)\n",
    "\n",
    "metadata = {\n",
    "    'outputs': [{\n",
    "        'type': 'confusion_matrix',\n",
    "        'format': 'csv',\n",
    "        'schema': [\n",
    "            {'name': 'target', 'type': 'CATEGORY'},\n",
    "            {'name': 'predicted', 'type': 'CATEGORY'},\n",
    "            {'name': 'count', 'type': 'NUMBER'},\n",
    "        ],\n",
    "        'source': cm_file,\n",
    "        # Convert vocab to string because for bealean values we want \"True|False\" to match csv data.\n",
    "        'labels': list(map(str, vocab)),\n",
    "    }]\n",
    "}\n",
    "try:\n",
    "    os.makedirs(os.path.join(DATA_DIR, \"artifacts\", \"confusionmatrix\"))\n",
    "except OSError:\n",
    "    pass\n",
    "with file_io.FileIO(os.path.join(DATA_DIR, 'artifacts', 'confusionmatrix', 'mlpipeline-ui-metadata.json'), 'w') as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "accuracy = accuracy_score(df['target'], df['predicted'])\n",
    "metrics = {\n",
    "    'metrics': [{\n",
    "        'name': 'accuracy-score',\n",
    "        'numberValue': accuracy,\n",
    "        'format': \"PERCENTAGE\",\n",
    "    }]\n",
    "}\n",
    "with file_io.FileIO(os.path.join(DATA_DIR, 'artifacts', 'confusionmatrix', 'mlpipeline-metrics.json'), 'w') as f:\n",
    "    json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ROC Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:roc",
     "prev:predict"
    ]
   },
   "outputs": [],
   "source": [
    "predictions = os.path.join(DATA_DIR, 'predict', 'prediction_results-*')\n",
    "target_lambda = \"\"\"lambda x: 1 if (x['target'] > x['fare'] * 0.2) else 0\"\"\"\n",
    "true_class = 'true'\n",
    "true_score_column = 'true'\n",
    "\n",
    "if not os.path.exists(os.path.join(DATA_DIR, \"roc\")):\n",
    "    os.makedirs(os.path.join(DATA_DIR, \"roc\"))\n",
    "\n",
    "schema_file = os.path.join(os.path.dirname(predictions), 'schema.json')\n",
    "schema = json.loads(file_io.read_file_to_string(schema_file))\n",
    "names = [x['name'] for x in schema]\n",
    "\n",
    "if not target_lambda and 'target' not in names:\n",
    "    raise ValueError('There is no \"target\" column, and target_lambda is not provided.')\n",
    "\n",
    "if true_score_column not in names:\n",
    "    raise ValueError('Cannot find column name \"%s\"' % true_score_column)\n",
    "\n",
    "dfs = []\n",
    "files = file_io.get_matching_files(predictions)\n",
    "for file in files:\n",
    "    with file_io.FileIO(file, 'r') as f:\n",
    "        dfs.append(pd.read_csv(f, names=names))\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "if target_lambda:\n",
    "    df['target'] = df.apply(eval(target_lambda), axis=1)\n",
    "else:\n",
    "    df['target'] = df['target'].apply(lambda x: 1 if x == true_class else 0)\n",
    "fpr, tpr, thresholds = roc_curve(df['target'], df[true_score_column])\n",
    "roc_auc = roc_auc_score(df['target'], df[true_score_column])\n",
    "df_roc = pd.DataFrame({'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds})\n",
    "roc_file = os.path.join(DATA_DIR + \"roc\", 'roc.csv')\n",
    "with file_io.FileIO(roc_file, 'w') as f:\n",
    "    df_roc.to_csv(f, columns=['fpr', 'tpr', 'thresholds'], header=False, index=False)\n",
    "\n",
    "metadata = {\n",
    "    'outputs': [{\n",
    "        'type': 'roc',\n",
    "        'format': 'csv',\n",
    "        'schema': [\n",
    "            {'name': 'fpr', 'type': 'NUMBER'},\n",
    "            {'name': 'tpr', 'type': 'NUMBER'},\n",
    "            {'name': 'thresholds', 'type': 'NUMBER'},\n",
    "        ],\n",
    "        'source': roc_file\n",
    "    }]\n",
    "}\n",
    "try:\n",
    "    os.makedirs(os.path.join(DATA_DIR, \"artifacts\", \"roc\"))\n",
    "except OSError:\n",
    "    pass\n",
    "with file_io.FileIO(os.path.join(DATA_DIR, 'artifacts', 'roc', 'mlpipeline-ui-metadata.json'), 'w') as f:\n",
    "    json.dump(metadata, f)\n",
    "\n",
    "metrics = {\n",
    "    'metrics': [{\n",
    "        'name': 'roc-auc-score',\n",
    "        'numberValue': roc_auc,\n",
    "    }]\n",
    "}\n",
    "with file_io.FileIO(os.path.join(DATA_DIR, 'artifacts', 'roc', 'mlpipeline-metrics.json'), 'w') as f:\n",
    "    json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deploy Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass: deploy would not be a \"local\" step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
